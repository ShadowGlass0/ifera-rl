{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E:\\Kibot Agent\\Data\\IBM.txt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"E:\\Kibot Agent\\Data\\IBM.txt\", header=None, names=[\"date\", \"time\", \"open\", \"high\", \"low\", \"close\", \"volume\"], dtype={\"open\": \"float32\", \"high\": \"float32\", \"low\": \"float32\", \"close\": \"float32\", \"volume\": \"float32\"})\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.date\n",
    "df[\"time\"] = pd.to_timedelta(df[\"time\"] + \":00\")\n",
    "\n",
    "start_time = pd.to_timedelta(\"09:30:00\")\n",
    "end_time = pd.to_timedelta(\"15:59:00\")\n",
    "time_step = pd.to_timedelta(\"00:01:00\") # 1 minute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the dates where the first time for the day is larger than 9:30 or the last time for the day is less than 15:59\n",
    "short_days = df.groupby('date').filter(lambda x: (x['time'].min() > start_time) | (x['time'].max() < end_time))['date'].unique()\n",
    "\n",
    "# remove the whole day's data for short days\n",
    "df = df[~df['date'].isin(short_days)]\n",
    "\n",
    "# print out all the dates which were removed\n",
    "print(\"The following dates were removed:\")\n",
    "# for date in short_days:\n",
    "#     print(date + \" \", df[df['Date'] == date]['Time'].min(), df[df['Date'] == date]['Time'].max())\n",
    "print(len(short_days))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = df['date'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_times = pd.timedelta_range(start=start_time, end=end_time, freq='1min')\n",
    "\n",
    "# Define a function to add missing rows to a group\n",
    "def add_missing_rows(group):\n",
    "    # Create a DataFrame with the missing rows\n",
    "    missing_rows = pd.DataFrame({'date': group['date'].iloc[0], 'time': missing_times, 'open': np.nan, 'high': np.nan, 'low': np.nan, 'close': np.nan, 'volume': np.nan})\n",
    "\n",
    "    # Merge the missing rows with the original group\n",
    "    merged = pd.merge(group, missing_rows, on=['date', 'time'], how='outer', suffixes=('', '_y'), sort=True)[group.columns]\n",
    "\n",
    "    # Fill in missing data: open, high, low, close = previous close, volume = 0\n",
    "    merged['close'] = merged['close'].ffill()\n",
    "    merged[['open', 'high', 'low', 'close']] = merged[['open', 'high', 'low', 'close']].bfill(axis=1)\n",
    "    merged['volume'] = merged['volume'].fillna(0)\n",
    "\n",
    "    return merged\n",
    "\n",
    "\n",
    "# Group the DataFrame by date and apply the function to each group\n",
    "df = df.groupby('date').apply(add_missing_rows).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('date').count().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.set_index('datetime').groupby('date').apply(lambda x: x.resample(time_step))\n",
    "\n",
    "pd.timedelta_range(start='09:30:00', end='15:59:00', freq='1min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = pd.DataFrame([\n",
    "            {'date': pd.to_datetime('2000-01-01'), 'time': start_time, 'open': 25.0, 'high': 25.5, 'low': 24.5, 'close': 25.2, 'volume': 1500},\n",
    "            {'date': pd.to_datetime('2000-01-01'), 'time': end_time, 'open': 26.0, 'high': 26.5, 'low': 25.8, 'close': 26.4, 'volume': 2000}\n",
    "        ])\n",
    "\n",
    "missing_times = pd.timedelta_range(start=start_time, end=end_time, freq='1min')\n",
    "missing_rows = pd.DataFrame({'date': group['date'].iloc[0], 'time': missing_times, 'open': np.nan, 'high': np.nan, 'low': np.nan, 'close': np.nan, 'volume': np.nan})\n",
    "\n",
    "merged = pd.merge(group, missing_rows, on=['date', 'time'], how='outer', suffixes=('', '_y'), sort=True)[group.columns]\n",
    "\n",
    "#merged = merged.sort_values('time')\n",
    "merged['close'] = merged['close'].ffill()\n",
    "merged[['open', 'high', 'low', 'close']] = merged[['open', 'high', 'low', 'close']].bfill(axis=1)\n",
    "merged['volume'] = merged['volume'].fillna(0)\n",
    "\n",
    "#merged.head()\n",
    "merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib as pl\n",
    "\n",
    "in_prefix = pl.Path(\"data\", \"raw\")\n",
    "instrument_type = \"futures\"\n",
    "interval = \"1m\"\n",
    "symbol = \"CL\"\n",
    "extension = \"txt\"\n",
    "day_start_time = \"-6:00:00\"\n",
    "day_end_time = \"17:00:00\"\n",
    "\n",
    "seconds_per_day = 24 * 60 * 60\n",
    "\n",
    "#if instrument_type == \"futures\":\n",
    "start_time_offset = pd.to_timedelta(day_start_time)\n",
    "start_time = pd.to_timedelta(\"01:00:00\")\n",
    "time_step = pd.to_timedelta(interval)\n",
    "end_time = pd.to_timedelta(day_end_time) - start_time_offset - time_step\n",
    "offset_seconds = start_time_offset.total_seconds()\n",
    "total_steps = int((end_time - start_time).total_seconds() / time_step.total_seconds()) + 1\n",
    "\n",
    "in_path = pl.Path(in_prefix, instrument_type, interval, symbol + \".\" + extension)\n",
    "\n",
    "df = pd.read_csv(in_path, header=None, names=[\"date\", \"time\", \"open\", \"high\", \"low\", \"close\", \"volume\"], parse_dates=[[\"date\", \"time\"]],\n",
    "                 dtype={\"open\": \"float32\", \"high\": \"float32\", \"low\": \"float32\", \"close\": \"float32\", \"volume\": \"int32\"}, index_col=\"date_time\")\n",
    "\n",
    "df['date'] =  pd.to_datetime(df.index.date)\n",
    "df['time'] =  pd.to_timedelta(df.index.hour * 3600 + df.index.minute * 60 + df.index.second, unit='s')\n",
    "\n",
    "df['offset_time'] = df['time'] - start_time_offset\n",
    "df['offset_time'] = df['offset_time'].apply(lambda x: x - pd.to_timedelta(x.days, unit='d'))\n",
    "\n",
    "# Calculte the trade_date column. Futures contracts start trading at 18:00 on the previous calendar day.\n",
    "df['trade_date'] = pd.to_datetime((df.index - start_time_offset).date)\n",
    "\n",
    "full_days = pd.read_csv(pl.Path(\"data\", \"full_days.csv\"), header=None, names=[\"date\"], parse_dates=[\"date\"], index_col=\"date\")\n",
    "\n",
    "df = df[df[\"trade_date\"].isin(full_days.index)]\n",
    "\n",
    "missing_times = pd.timedelta_range(start=start_time, end=end_time, freq=time_step)\n",
    "\n",
    "# Define a function to add missing rows to a group\n",
    "def add_missing_rows(group):\n",
    "    # Create a DataFrame with the missing rows\n",
    "    missing_rows = pd.DataFrame({'trade_date': group['trade_date'].iloc[0], 'offset_time': missing_times, 'open': np.nan, 'high': np.nan, 'low': np.nan, 'close': np.nan, 'volume': np.nan})\n",
    "\n",
    "    # Merge the missing rows with the original group\n",
    "    merged = pd.merge(group, missing_rows, on=['trade_date', 'offset_time'], how='outer', suffixes=('', '_y'), sort=True)[group.columns]\n",
    "\n",
    "    # Fill in missing data: open, high, low, close = previous close, volume = 0\n",
    "    merged['close'] = merged['close'].ffill()\n",
    "    merged[['open', 'high', 'low', 'close']] = merged[['open', 'high', 'low', 'close']].bfill(axis=1)\n",
    "    merged['volume'] = merged['volume'].fillna(0).astype('int32')\n",
    "\n",
    "    return merged\n",
    "\n",
    "# Group the DataFrame by date and apply the function to each group\n",
    "df = df.groupby('trade_date').apply(add_missing_rows).reset_index(drop=True)\n",
    "\n",
    "# Remove rows outside of the specified time range\n",
    "df = df[(df['offset_time'] >= start_time) & (df['offset_time'] <= end_time)]\n",
    "\n",
    "# Remove days where number of 'open' values is less than total_steps\n",
    "df = df.groupby('trade_date').filter(lambda x: x['open'].count() == total_steps)\n",
    "\n",
    "df.sort_values(['trade_date', 'offset_time'], inplace=True)\n",
    "\n",
    "df['time_seconds'] = (df['offset_time'].map(lambda x: x.total_seconds()) + offset_seconds).mod(seconds_per_day).astype('int32')\n",
    "df['ord_date'] = (df['trade_date'].map(lambda x: x.toordinal())).astype('int32')\n",
    "\n",
    "df = df[['ord_date', 'time_seconds', 'open', 'high', 'low', 'close', 'volume']].reset_index(drop=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(pl.Path(\"data\", \"processed\", instrument_type, interval, symbol + \".csv\"), index=False, float_format='%.2f', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as plt\n",
    "\n",
    "# Plot the close column values with the trade_date column as x axis\n",
    "df.plot(x='ord_date', y='close', figsize=(20,8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the first 5 rows where open is nan\n",
    "#df2[df2['open'].isnull()].head()\n",
    "\n",
    "df.groupby('trade_date').count().min(), df.groupby('trade_date').count().max()    \n",
    "\n",
    "# Print out dates where the number of 'open' values is not 1380\n",
    "print(\"The following dates have missing data:\")\n",
    "df.groupby('trade_date').count()[df.groupby('trade_date').count()['open'] < 1380]\n",
    "\n",
    "# write out to a file the full df on trade_date 2009-09-28\n",
    "# df[df['trade_date'] == pd.to_datetime('2015-07-01')].to_csv(\"data/2015-07-01.csv\")\n",
    "\n",
    "df.groupby('offset_time')['volume'].mean().to_csv(\"data/average_volume.csv\")\n",
    "#[df.groupby('trade_date')['offset_time'].min() > pd.to_timedelta('00:00:00')]\n",
    "\n",
    "# Filter out rows where open is nan\n",
    "#df = df[~df['open'].isnull()]\n",
    "\n",
    "#TODO: Remove rows after end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.to_datetime('2015-07-01 09:30:00') - pd.to_timedelta(\"-6:00:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from einops import rearrange\n",
    "from datetime import datetime, date, time, timedelta\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ord_dates = torch.from_numpy(np.unique(df['ord_date'])).to(device=device, dtype=torch.int32)\n",
    "# ord_dates = np.unique(df['ord_date'])\n",
    "\n",
    "# Load df into a torch tensor\n",
    "df_tensor = torch.from_numpy(df[['time_seconds', 'open', 'high', 'low', 'close', 'volume']].values).to(device=device, dtype=torch.float32)\n",
    "\n",
    "df_tensor = rearrange(df_tensor, '(d t) c -> d t c', t=total_steps) \n",
    "df_tensor.shape\n",
    "\n",
    "df_tensor[:, :, 1:].min(dim=1)[0].min(dim=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import iferarl.env_intraday as myenv\n",
    "from datetime import datetime, date, time, timedelta\n",
    "from torchrl.envs.utils import check_env_specs\n",
    "import pathlib as pl\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "importlib.reload(myenv)\n",
    "\n",
    "instrument_type = \"futures\"\n",
    "interval = \"1m\"\n",
    "symbol = \"CL\"\n",
    "extension = \"txt\"\n",
    "day_start_time = \"-6:00:00\"\n",
    "day_end_time = \"17:00:00\"\n",
    "start_time_offset = pd.to_timedelta(day_start_time)\n",
    "start_time = pd.to_timedelta(\"01:00:00\")\n",
    "time_step = pd.to_timedelta(interval)\n",
    "end_time = pd.to_timedelta(day_end_time) - start_time_offset - time_step\n",
    "offset_seconds = start_time_offset.total_seconds()\n",
    "total_steps = int((end_time - start_time).total_seconds() / time_step.total_seconds()) + 1\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "file_path = pl.Path(\"data\", \"processed\", instrument_type, interval, symbol + \".csv\")\n",
    "\n",
    "# Convert start_time from pandas timedelta to datetime time\n",
    "start_time_t = time(hour=start_time.components.hours, minute=start_time.components.minutes, second=start_time.components.seconds)\n",
    "end_time_t = time(hour=end_time.components.hours, minute=end_time.components.minutes, second=end_time.components.seconds)\n",
    "\n",
    "env = myenv.IntradayEnv(file_path=file_path, start_time=start_time_t, end_time=end_time_t, time_step=time_step, test_start_date=date(2021, 1, 1), window_size=60, device=device)\n",
    "\n",
    "# td = env.reset()\n",
    "\n",
    "# td = env.rand_action(td)\n",
    "\n",
    "# td = env.step(td)\n",
    "# print(td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(datetime.combine(date(1, 1, 1), end_time_t) - datetime.combine(date(1, 1, 1), start_time_t)) // time_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.data.replay_buffers import ReplayBuffer, SamplerWithoutReplacement, RandomSampler\n",
    "from torchrl.data.replay_buffers.storages import TensorStorage\n",
    "\n",
    "tensor = torch.rand((1), device='cuda', dtype=torch.float32)\n",
    "d = torch.zeros((1), device='cuda', dtype=torch.float32)\n",
    "\n",
    "a = tensor / d\n",
    "\n",
    "a.nan_to_num(nan=0.0, posinf=0.0, neginf=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange, reduce, repeat\n",
    "\n",
    "time_idx = repeat(torch.tensor([60], dtype=torch.int64, device=device), '() -> 5')\n",
    "date_idx = torch.randint(0, 3496, (5,), dtype=torch.int64, device=device)\n",
    "windows_size = 60\n",
    "\n",
    "env.data.shape\n",
    "\n",
    "# Get a tensor based on env.data, where the first dimension is the date and the second dimension is the time range frim time_idx - windw_size to time_idx.\n",
    "iter_dl = torch.arange(0, windows_size, device=device)\n",
    "t1 = env.data[date_idx.unsqueeze(-1), iter_dl.unsqueeze(0), :]\n",
    "\n",
    "start_idx = time_idx - windows_size\n",
    "\n",
    "env.data[date_idx, time_idx[0]-windows_size:time_idx[0], :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.envs.libs.gym import GymEnv\n",
    "from torchrl.envs.transforms import TransformedEnv, Compose, UnsqueezeTransform, CatFrames\n",
    "\n",
    "env = TransformedEnv(GymEnv('Pendulum-v1'),\n",
    "     Compose(\n",
    "         UnsqueezeTransform(-1, in_keys=[\"observation\"]),\n",
    "         CatFrames(N=4, dim=-1, in_keys=[\"observation\"]),\n",
    "     )\n",
    ")\n",
    "\n",
    "env2 = GymEnv('Pendulum-v1')\n",
    "\n",
    "print(env.rollout(3))\n",
    "print(env2.rollout(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "@torch.compile(fullgraph=True, mode=\"max-autotune\")\n",
    "def testfn(a, b):\n",
    "    # zero_mask = (b == 0.0)\n",
    "    # non_zero_mask = ~zero_mask\n",
    "    # c = torch.empty_like(a)\n",
    "    # c[non_zero_mask] = a[non_zero_mask] / b[non_zero_mask]\n",
    "    # c[zero_mask] = 0.0\n",
    "\n",
    "    c = (a/b).nan_to_num(nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    c += b\n",
    "    c = c.log()\n",
    "    return c\n",
    "\n",
    "def testfn2(a, b):\n",
    "    # zero_mask = (b == 0.0)\n",
    "    # non_zero_mask = ~zero_mask\n",
    "    # c = torch.empty_like(a)\n",
    "    # c[non_zero_mask] = a[non_zero_mask] / b[non_zero_mask]\n",
    "    # c[zero_mask] = 0.0\n",
    "\n",
    "    c = (a/b).nan_to_num(nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    c += b\n",
    "    c = c.log()\n",
    "    return c\n",
    "\n",
    "def gen_test_data():\n",
    "    a = torch.rand((1000, 100000), dtype=torch.float32, device='cuda')\n",
    "    b = torch.rand((1000, 100000), dtype=torch.float32, device='cuda')\n",
    "    z = torch.randint_like(a, 0, 2, dtype=torch.bool, device='cuda')\n",
    "    b[z] = 0.0\n",
    "    return a, b\n",
    "\n",
    "a, x = gen_test_data()\n",
    "testfn(a, x)\n",
    "a, x = gen_test_data()\n",
    "testfn(a, x)\n",
    "a, x = gen_test_data()\n",
    "\n",
    "%timeit testfn2(a, b)\n",
    "%timeit testfn(a, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib as pl\n",
    "\n",
    "path = pl.Path(\"data\", \"\", \"futures\", \"1m\", \"CL\")\n",
    "path = path.with_suffix(\".csv\")\n",
    "\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open('data/instruments.json', 'r') as f:\n",
    "    main_data = json.load(f)\n",
    "\n",
    "instrument = main_data[\"CL@IBKR:1m\"]\n",
    "\n",
    "#print(data[\"CL@IBKR:1m\"])\n",
    "\n",
    "start_time_offset = pd.to_timedelta(instrument[\"tradingStart\"])\n",
    "offset_seconds = start_time_offset.total_seconds()\n",
    "\n",
    "df = pd.read_csv(\"data/processed/futures/1m/CL.csv\", header=None, names=[\"date\", \"time\", \"trade_date\", \"offset_time\", \"open\", \"high\", \"low\", \"close\", \"volume\"], dtype={\"date\": \"int32\", \"time\": \"int32\", \"trade_date\": \"int32\", \"offset_time\": \"int32\", \"open\": \"float32\", \"high\": \"float32\", \"low\": \"float32\", \"close\": \"float32\", \"volume\": \"int32\"}, parse_dates=False, index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ifera.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from einops import rearrange\n",
    "\n",
    "config = InstrumentConfig()\n",
    "instrument = config.get_config(\"CL@IBKR:1m\")\n",
    "\n",
    "time_step = instrument.timeStep\n",
    "start_time_offset = instrument.tradingStart\n",
    "start_time = instrument.skipStartTime\n",
    "end_time = instrument.tradingEnd - start_time_offset - time_step\n",
    "total_steps = int((end_time - start_time).total_seconds() / time_step.total_seconds()) + 1\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dates = torch.from_numpy(np.unique(df['trade_date'])).to(device=device, dtype=torch.int32)\n",
    "main_data = torch.from_numpy(df[['time', 'open', 'high', 'low', 'close', 'volume']].values).to(device=device, dtype=torch.float32)\n",
    "main_data = rearrange(main_data, '(d t) c -> d t c', t=total_steps) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "data2 = torch.empty((dates.shape[0], total_steps, 12), device=device, dtype=torch.float32)\n",
    "\n",
    "# Column 1,2 : sin(time) and cos(time) stretched to the range of 0 to 2pi from 0 to seconds_per_day\n",
    "seconds_per_day = 24 * 60 * 60\n",
    "time = main_data[:, :, 0]\n",
    "time = time / seconds_per_day * 2 * math.pi\n",
    "data2[:, :, 0] = torch.sin(time)\n",
    "data2[:, :, 1] = torch.cos(time)\n",
    "\n",
    "# Columns 3-5: log(high), log(low), log(close)\n",
    "data2[:, :, 2] = main_data[:, :, 2].log()\n",
    "data2[:, :, 3] = main_data[:, :, 3].log()\n",
    "data2[:, :, 4] = main_data[:, :, 4].log()\n",
    "\n",
    "# Calculate prev_close into a separate tensor. Use open from the df for the first time step of each day and close from the previous time step for the rest.\n",
    "prev_close = torch.empty((dates.shape[0], total_steps), device=device, dtype=torch.float32)\n",
    "prev_close[:, 0] = main_data[:, 0, 1]\n",
    "prev_close[:, 1:] = main_data[:, :-1, 4]\n",
    "\n",
    "# Columns 6-11: log(high/low), log(high/close), log(low/close), log(close/prev_close), log(high/prev_close), log(low/prev_close). Use division instead of subtraction to increase precision.\n",
    "data2[:, :, 5] = (main_data[:, :, 2] / main_data[:, :, 3]).log()\n",
    "data2[:, :, 6] = (main_data[:, :, 2] / main_data[:, :, 4]).log()\n",
    "data2[:, :, 7] = (main_data[:, :, 3] / main_data[:, :, 4]).log()\n",
    "data2[:, :, 8] = (main_data[:, :, 4] / prev_close).log()\n",
    "data2[:, :, 9] = (main_data[:, :, 2] / prev_close).log()\n",
    "data2[:, :, 10] = (main_data[:, :, 3] / prev_close).log()\n",
    "\n",
    "# Calculate average daily volume\n",
    "window_size = 60\n",
    "average_volume = torch.empty((dates.shape[0] + window_size - 1), device=device, dtype=torch.float32)\n",
    "average_volume[:window_size-1] = main_data[0, :, 5].mean()\n",
    "average_volume[window_size-1:] = main_data[:, :, 5].mean(dim=1)\n",
    "\n",
    "# Calculate a simple moving average daily volume for the last 60 days from average_volume.\n",
    "windows = average_volume.unfold(dimension=0, size=window_size, step=1)\n",
    "sma_volume = windows.mean(dim=1)\n",
    "\n",
    "# Column 12: log(volume) - log(sma_volume)\n",
    "data2[:, :, 11] = main_data[:, :, 5].log() - sma_volume.log().unsqueeze(1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_diff =  data2[:, :, 5:11].max()\n",
    "# Find where max_diff is in the data2 tensor\n",
    "max_diff_idx = torch.where(data2[:, :, 5:11] == max_diff)\n",
    "\n",
    "max_diff_idx\n",
    "\n",
    "# row 2633 in df\n",
    "df.iloc[2633]\n",
    "\n",
    "average_volume.shape, windows.shape, sma_volume.shape, data2.shape\n",
    "\n",
    "average_volume[50:70], sma_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date \n",
    "\n",
    "date.fromordinal(737536), pd.Timedelta(seconds=49680)\n",
    "\n",
    "\n",
    "\n",
    "math.log(66.57 / 66.52), data2[2633, 1128, 5], main_data[2633, 1128, 2] / main_data[2633, 1128, 3], main_data[2633, 1128, 2], main_data[2633, 1128, 3]\n",
    "#max_diff_idx\n",
    "\n",
    "2633 * total_steps + 1128\n",
    "\n",
    "df.iloc[2633 * total_steps + 1128]\n",
    "\n",
    "date.fromordinal(737536), pd.Timedelta(seconds=49680)\n",
    "\n",
    "dates[2633]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.compile(fullgraph=True, mode=\"max-autotune\")\n",
    "def test_fn(x, a):\n",
    "    return torch.where(torch.tensor(a > 0.0, dtype=torch.bool), x.clamp(min=x), x)\n",
    "\n",
    "t1 = torch.rand((1000, 1000), device='cuda', dtype=torch.float32)\n",
    "test_fn(t1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.data.replay_buffers import ReplayBuffer, SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import TensorStorage\n",
    "\n",
    "a = torch.arange(0, 100, device='cuda', dtype=torch.float32) + 1000\n",
    "\n",
    "b = torch.randperm(100, device='cuda', dtype=torch.int64)\n",
    "mask1 = b < 50\n",
    "a_bucket1 = a[mask1]\n",
    "\n",
    "# Get the original indices of the elements in a_bucket1\n",
    "i_bucket1 = torch.arange(0, 100, device='cuda', dtype=torch.int64)[mask1]\n",
    "\n",
    "i_bucket1\n",
    "\n",
    "storage = TensorStorage(i_bucket1)\n",
    "replay_buffer = ReplayBuffer(storage=storage, sampler=SamplerWithoutReplacement())\n",
    "\n",
    "replay_buffer.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import ifera\n",
    "import torch\n",
    "from torchrl.envs.utils import check_env_specs\n",
    "from datetime import timedelta, time\n",
    "\n",
    "importlib.reload(ifera)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "config = ifera.InstrumentConfig()\n",
    "instrument = config.get_config(\"CL@IBKR:1m\")\n",
    "\n",
    "dates, main_data = ifera.load_instrument_data_tensor(instrument, dtype=torch.float32, device=device)\n",
    "\n",
    "market_sim = ifera.MarketSimulatorIntraday(instrument = instrument, dates=dates, data=main_data)\n",
    "market_sim.start_day(torch.tensor([0, 1, 2], dtype=torch.int32, device=device))\n",
    "\n",
    "# action = torch.tensor([0, 1, -2], dtype=torch.int32, device=torch.device(\"cuda\"))\n",
    "# profit, position, avg_entry_price = market_sim.calculate_step(market_sim.date_idx, market_sim.time_idx, market_sim.position, 0.0, action)\n",
    "\n",
    "# action = torch.tensor([3, -2, 1], dtype=torch.int32, device=torch.device(\"cuda\"))\n",
    "# profit, position, avg_entry_price = market_sim.calculate_step(market_sim.date_idx, market_sim.time_idx + 1, position, avg_entry_price, action)\n",
    "\n",
    "# action = torch.tensor([2, 1, 1], dtype=torch.int32, device=torch.device(\"cuda\"))\n",
    "# profit, position, avg_entry_price = market_sim.calculate_step(market_sim.date_idx, market_sim.time_idx + 2, position, avg_entry_price, action)\n",
    "\n",
    "# print(profit, position, avg_entry_price)\n",
    "\n",
    "env = ifera.IntradayEnv(market_sim=market_sim, window_size=360)\n",
    "env.reward_scaling = 1.0\n",
    "\n",
    "#env = torch.compile(env, mode=\"max-autotune\")\n",
    "\n",
    "# check_env_specs(env, False)\n",
    "batch_size = len(dates)\n",
    "start_time_idx = int((instrument.liquidStart - instrument.tradingStart).total_seconds() / instrument.timeStep.total_seconds())\n",
    "end_time_idx = int((instrument.liquidEnd - instrument.tradingStart).total_seconds() / instrument.timeStep.total_seconds()) - 1\n",
    "steps = end_time_idx - start_time_idx + 1\n",
    "\n",
    "def test_env():\n",
    "    td = env.reset(None, batch_size=batch_size, start_time_idx=start_time_idx, end_time_idx=end_time_idx)\n",
    "    return env.rollout(steps, auto_reset=False, tensordict=td)\n",
    "\n",
    "rollout = test_env()\n",
    "#rollout = env.rollout(1320, auto_reset=False, tensordict=env.reset(None, batch_size=batch_size))\n",
    "\n",
    "size = 0\n",
    "for key in rollout.keys(include_nested=True, leaves_only=True):\n",
    "    size += rollout[key].numel() * rollout[key].element_size()\n",
    "\n",
    "print(f\"{size:,}, {rollout['balance'].shape}\")\n",
    "\n",
    "rollout = test_env()\n",
    "#rollout = env.rollout(1320, auto_reset=False, tensordict=env.reset(None, batch_size=batch_size))\n",
    "\n",
    "# num_cells = 256\n",
    "\n",
    "# import torch.nn as nn\n",
    "\n",
    "# actor_net = nn.Sequential(\n",
    "#     nn.LazyLinear(num_cells, device=device),\n",
    "#     nn.ReLU(),\n",
    "#     nn.LazyLinear(num_cells, device=device),\n",
    "#     nn.ReLU(),\n",
    "#     nn.LazyLinear(num_cells, device=device),\n",
    "#     nn.ReLU(),\n",
    "#     nn.LazyLinear(2 * env.action_spec.shape[-1], device=device),\n",
    "#     NormalParamExtractor(),\n",
    "# )\n",
    "\n",
    "#%timeit rollout = test_env()\n",
    "#%timeit rollout = env.rollout(1320, auto_reset=False, tensordict=env.reset(None, batch_size=batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = rollout[\"next\"][\"reward\"].sum(dim=0)\n",
    "\n",
    "mean = results.mean()\n",
    "neg_results = torch.where(results < 0.0, results, torch.tensor(0.0, device=device))\n",
    "semi_std = torch.sqrt((neg_results - mean).pow(2).mean())\n",
    "mean, semi_std, (mean + 1.0) / (semi_std + 1.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class DataWindow(nn.Module):\n",
    "    __constants__ = ['window_size', 'data']\n",
    "    \n",
    "    def __init__(self, window_size: torch.int64 , data: torch.Tensor):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        self.data = data\n",
    "\n",
    "    def forward(self, date_idx: torch.Tensor, time_idx: torch.int64) -> torch.Tensor:\n",
    "        return self.data[date_idx, time_idx - self.window_size:time_idx, :]\n",
    "    \n",
    "proc_data = ifera.pre_process_data(main_data)\n",
    "\n",
    "data_window = DataWindow(360, proc_data)\n",
    "\n",
    "d = rollout[\"date_idx\"][23]\n",
    "t1= rollout[\"time_idx\"][23][0].item()\n",
    "data_window(d, t1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange, pack\n",
    "\n",
    "a = torch.rand((20, 60, 16), device='cuda', dtype=torch.float32)\n",
    "b = torch.rand((20, 16), device='cuda', dtype=torch.float32)\n",
    "\n",
    "pack([a, b], 'b * c')[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import ifera\n",
    "import torch\n",
    "\n",
    "from torchrl.modules import ProbabilisticActor\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data.replay_buffers import ReplayBuffer, SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "from torchrl.objectives.value import GAE\n",
    "from torchrl.objectives import ClipPPOLoss\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from torchrl.envs.utils import ExplorationType, set_exploration_type\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# import torch._dynamo\n",
    "# torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "importlib.reload(ifera)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Suppress pytorch warnings\n",
    "import warnings\n",
    "import datetime\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE = torch.float32\n",
    "\n",
    "config = ifera.InstrumentConfig()\n",
    "instrument = config.get_config(\"CL@IBKR:1m\")\n",
    "\n",
    "dates, main_data = ifera.load_instrument_data_tensor(instrument, dtype=DTYPE, device=device)\n",
    "proc_data = ifera.pre_process_data(main_data)\n",
    "\n",
    "market_sim = ifera.MarketSimulatorIntraday(instrument = instrument, dates=dates, data=main_data)\n",
    "\n",
    "start_time_idx = int((instrument.liquidStart - instrument.tradingStart).total_seconds() / instrument.timeStep.total_seconds())\n",
    "end_time_idx = int((instrument.liquidEnd - instrument.tradingStart).total_seconds() / instrument.timeStep.total_seconds()) - 1\n",
    "steps = end_time_idx - start_time_idx\n",
    "\n",
    "env = ifera.IntradayEnv(market_sim=market_sim, batch_size=(32,), window_size=360, start_time_idx=start_time_idx, end_time_idx=end_time_idx, max_units=1, passive_penalty=10.0)\n",
    "\n",
    "n_dim = 32\n",
    "\n",
    "actor_net = ifera.ActorNetHidden(512, n_dim, device=device, dtype=DTYPE)\n",
    "policy_module = ifera.PolicyModule(env, n_dim, proc_data, actor_net)\n",
    "\n",
    "value_net = ifera.ActorNetHidden(512, n_dim, device=device, dtype=DTYPE)\n",
    "value_module = ifera.ValueModule(env, n_dim, proc_data, value_net)\n",
    "\n",
    "actor = ProbabilisticActor(module=policy_module, in_keys=\"logits\", spec=env.action_spec, distribution_class=torch.distributions.Categorical, return_log_prob=True)\n",
    "\n",
    "# Init Lazy Modules to get the correct shape for the input tensors\n",
    "td = env.reset()\n",
    "_ = actor(td)\n",
    "_ = value_module(td)\n",
    "_ = env.rollout(steps, actor)\n",
    "\n",
    "frames_per_batch = 32 * steps\n",
    "epochs = 8192\n",
    "total_frames = frames_per_batch * epochs\n",
    "sub_batch_size = 64\n",
    "batches_per_data = env.dates.shape[0] // env.batch_size.numel()\n",
    "collector = SyncDataCollector(env, actor, frames_per_batch=frames_per_batch, total_frames=total_frames, split_trajs=False, device=device, reset_at_each_iter=True)\n",
    "\n",
    "replay_buffer = ReplayBuffer(storage=LazyTensorStorage(frames_per_batch, device=device), sampler=SamplerWithoutReplacement())\n",
    "\n",
    "advantage_module = GAE(gamma=1.0, lmbda=0.95, value_network=value_module, average_gae=True, device=device)\n",
    "\n",
    "loss_module = ClipPPOLoss(actor=actor, critic=value_module, clip_epsilon=0.2, entropy_bonus=True, entropy_coef=0.1, critic_coef=0.5)\n",
    "\n",
    "optim = torch.optim.Adam(loss_module.parameters(), 1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=math.pow(0.001, 1.0 / epochs))\n",
    "entropy_coef_decay = math.pow(0.01, 1.0 / epochs)\n",
    "\n",
    "logs = defaultdict(list)\n",
    "pbar = tqdm(total=total_frames)\n",
    "eval_str = \"\"\n",
    "\n",
    "time_rollout = 0\n",
    "time_GAE = 0\n",
    "time_loss = 0\n",
    "time_backward = 0\n",
    "time_optim = 0 \n",
    "time_eval = 0\n",
    "\n",
    "# We iterate over the collector until it reaches the total number of frames it was\n",
    "# designed to collect:\n",
    "start_time = time.time()\n",
    "for iter_dl, tensordict_data in enumerate(collector):\n",
    "    time_rollout += time.time() - start_time\n",
    "    # we now have a batch of data to work with. Let's learn something from it.\n",
    "    for _ in range(1):\n",
    "        # We'll need an \"advantage\" signal to make PPO work.\n",
    "        # We re-compute it at each epoch as its value depends on the value\n",
    "        # network which is updated in the inner loop.\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            advantage_module(tensordict_data)\n",
    "        time_GAE += time.time() - start_time\n",
    "\n",
    "        data_view = tensordict_data.reshape(-1)\n",
    "\n",
    "        replay_buffer.extend(data_view)\n",
    "\n",
    "        for _ in range(frames_per_batch // sub_batch_size):\n",
    "            start_time = time.time()\n",
    "            subdata = replay_buffer.sample(sub_batch_size)\n",
    "            loss_vals = loss_module(subdata.to(device))\n",
    "            loss_value = (\n",
    "                loss_vals[\"loss_objective\"]\n",
    "                + loss_vals[\"loss_critic\"]\n",
    "                + loss_vals[\"loss_entropy\"]\n",
    "            )\n",
    "            time_loss += time.time() - start_time\n",
    "\n",
    "            # Optimization: backward, grad clipping and optim step\n",
    "            start_time = time.time()\n",
    "            loss_value.backward()\n",
    "            time_backward += time.time() - start_time\n",
    "            # this is not strictly mandatory but it's good practice to keep\n",
    "            # your gradient norm bounded\n",
    "            # torch.nn.utils.clip_grad_norm_(loss_module.parameters(), max_grad_norm)\n",
    "            start_time = time.time()\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "            time_optim += time.time() - start_time\n",
    "\n",
    "    logs[\"reward\"].append(tensordict_data[\"next\", \"reward\"].sum(1).mean().item())\n",
    "    pbar.update(tensordict_data.numel())\n",
    "    cum_reward_str = (\n",
    "        f\"average reward={logs['reward'][-1]: 4.4f} (sma={np.mean(logs['reward'][-batches_per_data:]): 4.4f})\"\n",
    "    )\n",
    "    logs[\"lr\"].append(optim.param_groups[0][\"lr\"])\n",
    "    lr_str = f\"lr policy: {logs['lr'][-1]: 4.6f} ent_coeff: {loss_module.entropy_coef: 4.6f}\"\n",
    "\n",
    "    if iter_dl % 1 == 0:\n",
    "        # We evaluate the policy once every 10 batches of data.\n",
    "        # Evaluation is rather simple: execute the policy without exploration\n",
    "        # (take the expected value of the action distribution) for a given\n",
    "        # number of steps (1000, which is our env horizon).\n",
    "        # The ``rollout`` method of the env can take a policy as argument:\n",
    "        # it will then execute this policy at each step.\n",
    "        start_time = time.time()\n",
    "        with set_exploration_type(ExplorationType.MODE), torch.no_grad():\n",
    "            # execute a rollout with the trained policy\n",
    "            eval_rollout = env.rollout(steps, actor)\n",
    "            logs[\"eval reward (sum)\"].append(\n",
    "                eval_rollout[\"next\", \"reward\"].sum(1).mean().item()\n",
    "            )\n",
    "            eval_str = (\n",
    "                f\"eval reward: {logs['eval reward (sum)'][-1]: 4.4f} \"\n",
    "                f\"(sma={np.mean(logs['eval reward (sum)'][-batches_per_data:]): 4.4f})\"\n",
    "            )\n",
    "            del eval_rollout\n",
    "        time_eval += time.time() - start_time\n",
    "    pbar.set_description(\", \".join([eval_str, cum_reward_str, lr_str]))\n",
    "\n",
    "    # We're also using a learning rate scheduler. Like the gradient clipping,\n",
    "    # this is a nice-to-have but nothing necessary for PPO to work.\n",
    "    scheduler.step()\n",
    "    loss_module.entropy_coef *= entropy_coef_decay\n",
    "\n",
    "    # Start time for the rollout (used for profiling). This should measure the collector iteration time.\n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "def format_time(seconds):\n",
    "    time_delta = datetime.timedelta(seconds=seconds)\n",
    "    return str(time_delta)\n",
    "\n",
    "print(\"time_rollout:\", format_time(time_rollout))\n",
    "print(\"time_GAE:\", format_time(time_GAE))\n",
    "print(\"time_loss:\", format_time(time_loss))\n",
    "print(\"time_backward:\", format_time(time_backward))\n",
    "print(\"time_optim:\", format_time(time_optim))\n",
    "print(\"time_eval:\", format_time(time_eval))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(np.clip(logs[\"reward\"], -0.1, None))\n",
    "plt.title(\"training rewards (average)\")\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(np.clip(logs[\"eval reward (sum)\"], -0.1, None))\n",
    "plt.title(\"Return (test)\")\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(logs[\"lr\"])\n",
    "plt.title(\"Learning rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "max_runs = env.dates.shape[0] // env.batch_size[0]\n",
    "total_reward = 0\n",
    "max_reward = np.inf * -1\n",
    "min_reward = np.inf\n",
    "\n",
    "with set_exploration_type(ExplorationType.MODE), torch.no_grad():\n",
    "    for iter_dl in range(max_runs):\n",
    "        rollout = env.rollout(steps, actor)\n",
    "        total_reward += rollout[\"next\", \"reward\"].sum().item()\n",
    "        max_reward = max(max_reward, rollout[\"next\", \"reward\"].sum(1).max().item())\n",
    "        min_reward = min(min_reward, rollout[\"next\", \"reward\"].sum(1).min().item())\n",
    "\n",
    "avg_reward = total_reward / (max_runs * env.batch_size[0])\n",
    "\n",
    "avg_reward, max_reward, min_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout = env.rollout(steps, actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_dl = rollout[\"next\", \"reward\"].sum(1).argmax().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumsum = rollout[\"next\", \"reward\"][iter_dl].cumsum(0).cpu().numpy()\n",
    "position = rollout[\"next\", \"position\"][iter_dl].cpu().numpy() -5\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(cumsum)\n",
    "plt.title(\"Cumulative Reward\")\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(position)\n",
    "plt.title(\"Position\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import itertools\n",
    "\n",
    "d = torch.arange(0, 20, device='cuda', dtype=torch.float32)\n",
    "\n",
    "ds = TensorDataset(d)\n",
    "\n",
    "dl = DataLoader(ds, batch_size=25, shuffle=True, drop_last=True)\n",
    "\n",
    "# infinite_iterator = itertools.cycle(dl)\n",
    "\n",
    "# for _ in range(20):\n",
    "#     batch = next(infinite_iterator)\n",
    "#     print(batch)\n",
    "\n",
    "for d in enumerate(dl):\n",
    "    print(d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = torch.randint(3600, 82740, (10, 1000), device='cuda', dtype=torch.int64)\n",
    "min_time = time.min(dim=1, keepdim=True)[0]\n",
    "max_time = time.max(dim=1, keepdim=True)[0] + 60\n",
    "time = (time - min_time) / (max_time - min_time)\n",
    "time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import ifera_n as ifera\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from einops import rearrange, reduce, repeat\n",
    "\n",
    "# import torch._dynamo\n",
    "# torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "importlib.reload(ifera)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Suppress pytorch warnings\n",
    "import warnings\n",
    "import datetime\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE = torch.float32\n",
    "\n",
    "config = ifera.InstrumentConfig()\n",
    "instrument = config.get_config(\"CL@IBKR:1m\")\n",
    "\n",
    "dates, main_data = ifera.load_instrument_data_tensor(instrument, dtype=DTYPE, device=device)\n",
    "\n",
    "max_units = 5\n",
    "n_positions = 2 * max_units + 1\n",
    "\n",
    "start_time_idx = int((instrument.liquidStart - instrument.tradingStart - instrument.skipStartTime).total_seconds() / instrument.timeStep.total_seconds())\n",
    "end_time_idx = int((instrument.liquidEnd - instrument.tradingStart - instrument.skipStartTime).total_seconds() / instrument.timeStep.total_seconds()) - 1\n",
    "steps = end_time_idx - start_time_idx + 1\n",
    "\n",
    "profit_tab = torch.zeros((main_data.shape[0], steps, n_positions, n_positions), device=device, dtype=DTYPE)\n",
    "optimal_values = torch.full((main_data.shape[0], steps, n_positions), -np.inf, device=device, dtype=DTYPE)\n",
    "worst_values = torch.full((main_data.shape[0], steps, n_positions), np.inf, device=device, dtype=DTYPE)\n",
    "past_profit_best = torch.full((main_data.shape[0], steps, n_positions), -np.inf, device=device, dtype=DTYPE)\n",
    "past_profit_worst = torch.full((main_data.shape[0], steps, n_positions), np.inf, device=device, dtype=DTYPE)\n",
    "\n",
    "market_sim = ifera.MarketSimulatorIntraday(instrument = instrument, dates=dates, data=main_data)\n",
    "date_idx = torch.arange(0, main_data.shape[0], device=device, dtype=torch.int64)\n",
    "pos = torch.arange(-max_units, max_units + 1, device=device, dtype=torch.int32)\n",
    "target_pos = torch.arange(-max_units, max_units + 1, device=device, dtype=torch.int32)\n",
    "\n",
    "date_idx = repeat(date_idx, 'd -> d p1 p2', p1=n_positions, p2=n_positions)\n",
    "pos = repeat(pos, 'p -> d p p2', d=date_idx.shape[0], p2=n_positions)\n",
    "target_pos = repeat(target_pos, 'p2 -> d p p2', d=date_idx.shape[0], p=n_positions)\n",
    "action = target_pos - pos\n",
    "\n",
    "day_open_price = main_data[:, start_time_idx, 1]\n",
    "margin_multiplier = instrument.margin / (instrument.contractMultiplier * instrument.referencePrice)\n",
    "reference_capital = day_open_price * instrument.contractMultiplier * max_units #* margin_multiplier\n",
    "reference_capital = repeat(reference_capital, 'd -> d p p2', p=n_positions, p2=n_positions)\n",
    "\n",
    "# Pre-calculate all profits\n",
    "for i in tqdm(range(steps)):\n",
    "    profit, _ = market_sim.calculate_step(date_idx, i + start_time_idx, pos, action)\n",
    "    profit = (profit / reference_capital)\n",
    "    profit_tab[:, i, :, :] = profit\n",
    "\n",
    "# Last time step, the action is fixed to 0 target_position (close all positions)\n",
    "profit = profit_tab[:, -1, :, :]\n",
    "profit = repeat(profit[:, :, max_units], 'd p -> d p p2', p=n_positions, p2=n_positions)\n",
    "optimal_values[:, -1, :] = (profit).max(dim=2)[0]\n",
    "worst_values[:, -1, :] = (profit).min(dim=2)[0]\n",
    "\n",
    "# Loop backwards through the time steps\n",
    "for i in tqdm(range(steps - 2, -1, -1)):\n",
    "    profit = profit_tab[:, i, :, :]\n",
    "    optimal_values[:, i, :] = (profit + optimal_values[date_idx, i + 1, target_pos + max_units]).max(dim=2)[0]\n",
    "    worst_values[:, i, :] = (profit + worst_values[date_idx, i + 1, target_pos + max_units]).min(dim=2)[0]\n",
    "\n",
    "past_profit_best[:, 0, :] = 0.0\n",
    "past_profit_worst[:, 0, :] = 0.0\n",
    "\n",
    "for i in tqdm(range(0, steps - 1)):\n",
    "    profit = profit_tab[:, i, :, :]\n",
    "    past_profit_best[:, i + 1, :] = (profit + past_profit_best[date_idx, i, pos + max_units]).max(dim=1)[0]\n",
    "    past_profit_worst[:, i + 1, :] = (profit + past_profit_worst[date_idx, i, pos + max_units]).min(dim=1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimal_values[1001, 501, :] + (market_sim.calculate_step(1000, 500, pos[1000, 5, :], action[1000, 5, :])[0] / reference_capital[1000, 5, :]).max(dim=2)[0]\n",
    "d = 1001\n",
    "((market_sim.calculate_step(d, 500 + start_time_idx, pos[d, 5, :], action[d, 5, :])[0] / reference_capital[d, 5, :]) + optimal_values[d, 501, :]).max() - optimal_values[d, 500, 5]\n",
    "((market_sim.calculate_step(d, 500 + start_time_idx, pos[d, :, 5], action[d, :, 5])[0] / reference_capital[d, :, 5]) + past_profit_best[d, 500, :]).max() - past_profit_best[d, 501, 5]\n",
    "\n",
    "#print(f\"{optimal_values.numel() * optimal_values.element_size() * 11:,}\")\n",
    "\n",
    "#start_time_idx, end_time_idx, steps\n",
    "\n",
    "torch.max(-(past_profit_worst + worst_values).min(), (past_profit_best + optimal_values).max())\n",
    "\n",
    "profit_tab[:, :, :, :].shape\n",
    "\n",
    "batch_size = 32\n",
    "date_batch = torch.randint(0, profit.shape[0], (batch_size,), device=device, dtype=torch.int64)\n",
    "time_batch = torch.randint(0, profit.shape[1], (batch_size,), device=device, dtype=torch.int64)\n",
    "pos_batch = torch.randint(0, profit.shape[2], (batch_size,), device=device, dtype=torch.int64)\n",
    "\n",
    "profit_batch = profit_tab[date_batch, time_batch, pos_batch, :]\n",
    "target_pos_idx = repeat(torch.arange(0, n_positions, device=device, dtype=torch.int64), 'p -> b p', b=batch_size)\n",
    "profit_batch + optimal_values[date_batch, time_batch + 1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(past_profit_best + optimal_values).max(dim=2)[0].min()\n",
    "\n",
    "torch.unravel_index((past_profit_best + optimal_values).max(dim=2)[0].argmin(), optimal_values.shape[:2])\n",
    "\n",
    "(past_profit_best +optimal_values)[1144].max(dim=1)[0]\n",
    "\n",
    "\n",
    "\n",
    "#main_data[1145, start_time_idx:end_time_idx, 1].min(), main_data[1145, start_time_idx:end_time_idx, 1].max(), (main_data[1145, start_time_idx:end_time_idx, 5] == 0.0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(market_sim.calculate_step(1145, 0 + start_time_idx, pos[1145], action[1145])[0] / reference_capital).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "# optimal_values[1145, :, 5]\n",
    "\n",
    "# main_data[1145, start_time_idx:end_time_idx, 4]\n",
    "\n",
    "t1 = optimal_values.shape[1] //4\n",
    "\n",
    "# optimal_values[:, t, :].min(), optimal_values[:, t, :].max(), optimal_values[:, t, :].mean(), optimal_values[:, t, :].std()\n",
    "\n",
    "#optimal_values[:, 0, :].mean(), optimal_values[:, 0, :].std()\n",
    "\n",
    "#ov = optimal_values[:, t, :]\n",
    "ov = past_profit_best[:, t1, :]\n",
    "\n",
    "#ov = (ov - ov.mean()) / ov.std()\n",
    "\n",
    "ov = (ov.flatten() + 0.01).log().cpu().numpy()\n",
    "#ov = (ov.flatten()).cpu().numpy()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import scipy.stats as stats\n",
    "\n",
    "ov_min = ov.min()\n",
    "ov_max = ov.max()\n",
    "ov_mean = ov.mean()\n",
    "ov_std = ov.std()\n",
    "\n",
    "x = np.linspace(ov_min, ov_max, 1000)\n",
    "y = stats.norm.pdf(x, ov_mean, ov_std)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.hist(ov, bins=500, density=True, alpha=0.7, label='Histogram')\n",
    "plt.plot(x, y, 'r', label='Normal Distribution')\n",
    "\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of ov Tensor Values with Normal Distribution')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Fit a T-distribution to the ov data series\n",
    "# df, loc, scale = stats.t.fit(ov)\n",
    "\n",
    "# # Generate the T-distribution PDF\n",
    "# x = np.linspace(ov_min, ov_max, 1000)\n",
    "# y = stats.t.pdf(x, df, loc, scale)\n",
    "\n",
    "\n",
    "# # Plot the histogram and T-distribution\n",
    "# plt.figure(figsize=(15, 5))\n",
    "# plt.hist(ov, bins=1000, density=True, alpha=0.7, label='Histogram')\n",
    "# plt.plot(x, y, 'r', label='T-Distribution')\n",
    "\n",
    "# plt.xlabel('Value')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.title('Histogram of ov Tensor Values with T-Distribution')\n",
    "# plt.legend()\n",
    "\n",
    "# #plt.xlim(0, 1.0)\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(-worst_values.min(), optimal_values.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "path = ifera.make_path(True, instrument)\n",
    "mtime = datetime.datetime.fromtimestamp(path.stat().st_mtime)\n",
    "mtime\n",
    "type(path.stat().st_mtime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import ifera_n as ifera\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "config = ifera.InstrumentConfig()\n",
    "instrument = config.get_config(\"CL@IBKR:1m\")\n",
    "start_time_offset = instrument.tradingStart\n",
    "\n",
    "df = ifera.load_data(raw=True, instrument=instrument)\n",
    "df['trade_date'] = pd.to_datetime((df.index - start_time_offset).date)\n",
    "rs = df.resample('30min')\n",
    "df2 = rs[['volume']].sum()\n",
    "df2['trade_date'] = pd.to_datetime((df2.index - start_time_offset).date)\n",
    "df2 = df2[(df2.index.time <= dt.time(15, 30))|(df2.index.time >= dt.time(19, 00))]\n",
    "\n",
    "rs2 = df2.resample('D', on='trade_date')\n",
    "df3 = rs2[['volume', 'trade_date']].min()\n",
    "\n",
    "df3[df3['volume'] == 0]\n",
    "#df2[df2['trade_date'] == pd.to_datetime('2009-10-03')]\n",
    "\n",
    "\n",
    "df3[(df3['volume'] == 0)&(df3['trade_date'].dt.dayofweek <= 4)]\n",
    "df2[df2['trade_date'] == pd.to_datetime('2009-10-20')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import ifera_n as ifera\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import copy\n",
    "\n",
    "importlib.reload(ifera)\n",
    "\n",
    "pd.set_option('display.width', 160)\n",
    "\n",
    "config = ifera.InstrumentConfig()\n",
    "instrument = config.get_config(\"CL@IBKR:1m\")\n",
    "\n",
    "# time_step = instrument.timeStep\n",
    "# start_time_offset = instrument.tradingStart\n",
    "# start_time = instrument.skipStartTime\n",
    "# end_time = instrument.endTime\n",
    "# total_steps = instrument.totalSteps\n",
    "# offset_seconds = start_time_offset.total_seconds()\n",
    "\n",
    "# df = ifera.load_data(raw=True, instrument=instrument, dtype=\"float64\")\n",
    "\n",
    "# df['date'] =  pd.to_datetime(df.index.date)\n",
    "# df['time'] =  pd.to_timedelta(df.index.hour * 3600 + df.index.minute * 60 + df.index.second, unit='s')\n",
    "\n",
    "# df['offset_time'] = df['time'] - start_time_offset\n",
    "# df['offset_time'] = df['offset_time'].apply(lambda x: x - pd.to_timedelta(x.days, unit='d'))\n",
    "\n",
    "# df['trade_date'] = pd.to_datetime((df.index - start_time_offset).date)\n",
    "\n",
    "path = ifera.make_path(raw=False, instrument=instrument, special_interval=\"fulldays\")\n",
    "\n",
    "instrument30min = copy.deepcopy(instrument)\n",
    "instrument30min.interval = \"30m\"\n",
    "instrument30min.__post_init__()\n",
    "time_steps_seconds = instrument30min.timeStep.total_seconds()\n",
    "\n",
    "df = ifera.load_data(raw=True, instrument=instrument30min)\n",
    "\n",
    "df['time'] = pd.to_timedelta(df.index.hour * 3600 + df.index.minute * 60 + df.index.second, unit='s')\n",
    "\n",
    "start_time_offset = instrument30min.tradingStart\n",
    "start_time = instrument30min.skipStartTime\n",
    "end_time = instrument30min.liquidEnd - instrument30min.tradingStart - instrument30min.timeStep\n",
    "\n",
    "df['trade_date'] = pd.to_datetime((df.index - start_time_offset).date)\n",
    "df['offset_time'] = df['time'] - start_time_offset\n",
    "df['offset_time'] = df['offset_time'].apply(lambda x: x - pd.to_timedelta(x.days, unit='d'))\n",
    "\n",
    "df = df[(df['offset_time'] >= start_time) & (df['offset_time'] <= end_time)]\n",
    "\n",
    "dates = df.groupby('trade_date').agg({'offset_time': ['count']})\n",
    "max_steps = int((end_time - start_time).total_seconds() / time_steps_seconds) + 1\n",
    "\n",
    "short_days = dates[(dates['offset_time']['count'] < max_steps)]\n",
    "full_days = dates.drop(short_days.index, errors='ignore').reset_index()[['trade_date']]\n",
    "short_days = short_days.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ifera.InstrumentConfig()\n",
    "instrument = config.get_config(\"CL@IBKR:1m\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "loss_matrix = optimal_values[:, :-1, :].max(dim=-1, keepdim=True)[0] - optimal_values[:, :-1, :]\n",
    "\n",
    "loss_matrix = loss_matrix / loss_matrix.std()\n",
    "\n",
    "loss_matrix.min(), loss_matrix.max(), loss_matrix.mean(), loss_matrix.std(), (loss_matrix > (loss_matrix.std() * 20 + loss_matrix.mean())).sum(), loss_matrix.numel()\n",
    "\n",
    "# loss_square = (loss_matrix + 1.0).pow(2) - 1.0\n",
    "# loss_square = loss_square / loss_square.std()\n",
    "\n",
    "# loss_square.min(), loss_square.max(), loss_square.mean(), loss_square.std(), (loss_square > (loss_square.std() * 20 + loss_square.mean())).sum(), loss_square.numel()\n",
    "\n",
    "# zero_vol = (main_data[:, start_time_idx+1:end_time_idx, 5] == 0.0)\n",
    "# loss_matrix2 = loss_matrix.clone().detach()\n",
    "# loss_matrix2[~zero_vol] = 0.0\n",
    "\n",
    "# lm = torch.unravel_index(loss_matrix2.argmax(), loss_matrix2.shape)\n",
    "\n",
    "# dt.date.fromordinal(dates[lm[0]]), dt.timedelta(minutes=lm[1].item()) + instrument.liquidStart, loss_matrix2.max(), (loss_matrix > loss_matrix2.max()).sum()\n",
    "\n",
    "# p = loss_matrix.max(dim=-1)[0].max(dim=-1)[0].sort(descending=True)[0].cpu()\n",
    "# p = loss_matrix.flatten()\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# #non_zero_p = (p[p != 0.0] + 0.01).log().cpu()\n",
    "# non_zero_p = p[p != 0.0].cpu()\n",
    "\n",
    "# plt.hist(non_zero_p, bins=1000)\n",
    "# plt.xlabel('Value')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.title('Histogram of p (non-zero values)')\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{:,}\".format(p.numel() * p.element_size()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(main_data[:, :, 5] == 0.0).any(dim=1).sum()\n",
    "\n",
    "zero_vol = (main_data[:, start_time_idx+1:end_time_idx, 5] == 0.0)\n",
    "\n",
    "print(zero_vol[:, 0:10].sum(dim=0))\n",
    "\n",
    "# Shift zero_vol by 1 on dim 1 to get the previous time step\n",
    "zero_vol = zero_vol.roll(shifts=-1, dims=1)\n",
    "\n",
    "zero_vol.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import ifera_n as ifera\n",
    "import datetime as dt\n",
    "import pathlib as pl\n",
    "\n",
    "importlib.reload(ifera)\n",
    "\n",
    "config = ifera.InstrumentConfig()\n",
    "instrument = config.get_config(\"CL@IBKR:1m\")\n",
    "\n",
    "path = ifera.make_path(raw=False, instrument=instrument)\n",
    "last_update = path.stat().st_mtime\n",
    "raw_path = ifera.make_path(raw=True, instrument=instrument)\n",
    "\n",
    "last_update, raw_path.stat().st_mtime, instrument.last_update, last_update < raw_path.stat().st_mtime or last_update < instrument.last_update\n",
    "\n",
    "# (dates, data) = ifera.load_instrument_data_tensor(instrument, dtype=torch.float32, device=torch.device(\"cuda\"))\n",
    "\n",
    "# dates\n",
    "# columns = [\"date\", \"time\", \"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "# dtype = {\"open\": dtype, \"high\": dtype, \"low\": dtype, \"close\": dtype, \"volume\": \"int32\"}\n",
    "# df = pd.read_csv(raw_path, header=None, parse_dates=False, names=columns, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import ifera_n as ifera\n",
    "import datetime as dt\n",
    "import pathlib as pl\n",
    "\n",
    "importlib.reload(ifera)\n",
    "\n",
    "config = ifera.InstrumentConfig()\n",
    "instrument = config.get_config(\"CL@IBKR:1m\")\n",
    "\n",
    "(dates, data) = ifera.load_instrument_data_tensor(instrument, dtype=torch.float32, device=torch.device(\"cuda\"))\n",
    "\n",
    "#(dates == dt.date(2020, 4, 2).toordinal()).any()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(1, 10000, 10000)\n",
    "\n",
    "s = 0.001\n",
    "t1 = 1 - 10.0 / len(x)\n",
    "q = 1 - (1 - t1) / 10.0\n",
    "\n",
    "y = np.power(0.999, x) * s\n",
    "y2 = np.power(t1, x) * s\n",
    "\n",
    "for i in range(1, len(x)):\n",
    "    y[i] = y[i-1] * t1\n",
    "    t1 = 1.0 - (1.0 - t1) * q\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(x, y, 'r', label='Slowing Exponential')\n",
    "plt.plot(x, y2, 'g', label='Exponential')\n",
    "\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('LR')\n",
    "plt.legend()\n",
    "\n",
    "# Label the last value\n",
    "last_value = y[-1]\n",
    "plt.text(x[-1], last_value, f'{last_value:.8f}', ha='right', va='bottom')\n",
    "plt.text(x[5000], y[5000], f'{y[5000]:.8f}', ha='right', va='bottom')\n",
    "\n",
    "# Set the Y range to [0,1]\n",
    "plt.ylim(0, 0.001)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(y[0] / y[len(x)//2-1], y[len(x)//2] / y[-1], y[0] / y[-1])\n",
    "print(y2[0] / y2[len(x)//2-1], y2[len(x)//2] / y2[-1], y2[0] / y2[-1])\n",
    "print([y[0] / y2[0]], [y[len(x)//2] / y2[len(x)//2]], [y[-1] / y2[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.device(\"cuda:0\") if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import ifera_n as ifera\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "importlib.reload(ifera)\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "# os.environ['TORCH_LOGS'] = \"+dynamo\"\n",
    "# os.environ['TORCHDYNAMO_VERBOSE'] = \"1\"\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "config = ifera.InstrumentConfig()\n",
    "instrument = config.get_config(\"CL@IBKR:1m\")\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE = torch.float32\n",
    "MODEL_DTYPE = torch.float32\n",
    "D_MODEL = 64\n",
    "\n",
    "dates, main_data = ifera.load_instrument_data_tensor(instrument, dtype=DTYPE, device=device)\n",
    "\n",
    "market_sim = ifera.MarketSimulatorIntraday(instrument = instrument, dates=dates, data=main_data)\n",
    "env = ifera.IntradayEnv(market_sim=market_sim, batch_size=(main_data.shape[0],), window_size=60, start_time_idx=0, max_units=5)\n",
    "\n",
    "#actornet_hidden = ifera.ActorNetHidden(512, device=device, dtype=model_dtype)\n",
    "actornet_hidden = nn.TransformerEncoder(nn.TransformerEncoderLayer(D_MODEL, 8, 512, batch_first=True, activation='gelu', device=device, dtype=MODEL_DTYPE), num_layers=6)\n",
    "actor_net = ifera.ActorNet(env, D_MODEL, actornet_hidden)\n",
    "\n",
    "with torch.no_grad():\n",
    "    start_time = time.time()\n",
    "    rewards = env.rollout_all(actor_net, env.steps + 1)\n",
    "    print(f\"Execution time: {time.time() - start_time} seconds\")\n",
    "\n",
    "    # start_time = time.time()\n",
    "    # rewards = env.rollout_all(actor_net, env.steps + 1)\n",
    "    # print(f\"Execution time: {time.time() - start_time} seconds\")\n",
    "\n",
    "    # start_time = time.time()\n",
    "    # rewards = env.rollout_all(actor_net, env.steps + 1)\n",
    "    # print(f\"Execution time: {time.time() - start_time} seconds\")\n",
    "\n",
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "# sortino = (rewards.mean() + 1.0) / (rewards[rewards < 0.0].std() + 1.0) - 1.0\n",
    "\n",
    "# rewards.min(), rewards.max(), rewards.mean(), rewards.std(), sortino\n",
    "\n",
    "# data_idx = torch.empty(main_data.shape[0:2] + (2,), device=device, dtype=torch.int64)\n",
    "\n",
    "# data_idx[:, :, 0] = torch.arange(0, main_data.shape[0], device=device, dtype=torch.int64).unsqueeze(1).expand(-1, main_data.shape[1])\n",
    "# data_idx[:, :, 1] = torch.arange(0, main_data.shape[1], device=device, dtype=torch.int64).unsqueeze(0).expand(main_data.shape[0], -1)\n",
    "\n",
    "# data_idx = rearrange(data_idx, 'd t c -> (d t) c')\n",
    "\n",
    "# i = torch.randint(0, data_idx.shape[0], (32,), device=device, dtype=torch.int64)\n",
    "\n",
    "actor_net_train = ifera.ActorNet(env, 32, actornet_hidden, dist_return='probs')\n",
    "\n",
    "# print(f\"{data_idx.numel() * data_idx.element_size() / 1024 / 1024:.2f} MB\")\n",
    "upper_bound = main_data.shape[0] * env.steps * env.n_position\n",
    "print(f\"{upper_bound:,}\")\n",
    "\n",
    "r = torch.randint(0, upper_bound, (32,), device=device, dtype=torch.int64)\n",
    "date_idx, time_idx, pos_idx = r // (env.steps * env.n_position), (r % (env.steps * env.n_position)) // env.n_position, r % env.n_position\n",
    "\n",
    "x = actor_net_train(date_idx, time_idx + env.start_time_idx, pos_idx)\n",
    "\n",
    "x.shape\n",
    "\n",
    "\n",
    "#!!!TODO: time_idx is not scalar, it's a tensor of shape (batch_size) and it's not being broadcasted correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import ifera_n as ifera\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "importlib.reload(ifera)\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "DTYPE = torch.float32\n",
    "MODEL_DTYPE = torch.float32\n",
    "D_MODEL = 64\n",
    "MEAN_LOSS_WINDOW = 10000\n",
    "LR = 2e-5\n",
    "EXP_LR_DECAY_PER_EPOCH = 0.8\n",
    "EPOCHS = 5\n",
    "BATCCH_SIZE = 128\n",
    "MAX_UNITS = 5\n",
    "WINDOW_SIZE = 120\n",
    "RNG_SEED = 65\n",
    "\n",
    "# os.environ['TORCH_LOGS'] = \"+dynamo\"\n",
    "# os.environ['TORCHDYNAMO_VERBOSE'] = \"1\"\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "config = ifera.InstrumentConfig()\n",
    "instrument = config.get_config(\"CL@IBKR:1m\")\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "if RNG_SEED is not None:\n",
    "    torch.random.manual_seed(RNG_SEED)\n",
    "else:\n",
    "    torch.random.seed()\n",
    "\n",
    "dates, main_data = ifera.load_instrument_data_tensor(instrument, dtype=DTYPE, device=device)\n",
    "\n",
    "# overfit_day = 3087\n",
    "# dates = dates[overfit_day:overfit_day+1]\n",
    "# main_data = main_data[overfit_day:overfit_day+1]\n",
    "\n",
    "market_sim = ifera.MarketSimulatorIntraday(instrument = instrument, dates=dates, data=main_data)\n",
    "env = ifera.IntradayEnv(market_sim=market_sim, batch_size=(main_data.shape[0],), window_size=WINDOW_SIZE, start_time_idx=0, max_units=MAX_UNITS)\n",
    "\n",
    "#actornet_hidden = ifera.ActorNetHidden(512, device=device, dtype=model_dtype)\n",
    "actornet_hidden = nn.TransformerEncoder(nn.TransformerEncoderLayer(D_MODEL, 4, 1024, batch_first=True, activation='gelu', device=device, dtype=MODEL_DTYPE), num_layers=6)\n",
    "actor_net = ifera.ActorNet(env, D_MODEL, actornet_hidden)\n",
    "\n",
    "profit_manager = ifera.ProfitManager(env, cache_profits=True)\n",
    "value_manager = ifera.ValueManager(env, profit_manager, risk_factor_mult=1.5)\n",
    "loss = ifera.OptimalValueLoss(value_manager, cache_losses=True, model_dtype=MODEL_DTYPE)\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "@torch.compile(mode=\"max-autotune\")\n",
    "def train_step(r, env, actor_net_train, loss, optim):\n",
    "    date_idx, time_idx, pos_idx = r // (env.steps * env.n_position), (r % (env.steps * env.n_position)) // env.n_position, r % env.n_position\n",
    "    x = actor_net_train(date_idx, time_idx + env.start_time_idx, pos_idx)\n",
    "    l = loss(date_idx, time_idx, pos_idx, x)\n",
    "\n",
    "    l.backward()\n",
    "\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "\n",
    "    # if actor_net_train.env_out_transform.lin_data._parameters['weight'].isnan().any():\n",
    "    #     raise Exception(\"NaN in linear layer\")\n",
    "    # else:\n",
    "    #     print(\".\", end='')\n",
    "\n",
    "    return l.clone().detach()\n",
    "\n",
    "\n",
    "def train_epoch(batch_size: int, env: ifera.IntradayEnv, actor_net_train: nn.Module, loss, optim, pbar):\n",
    "    upper_bound = env.dates.shape[0] * env.steps * env.n_position\n",
    "    t = torch.randperm(upper_bound, device=device, dtype=torch.int64)\n",
    "    t_range = torch.arange(0, upper_bound - batch_size, batch_size, device=device, dtype=torch.int64)\n",
    "    losses = torch.zeros(((upper_bound // batch_size) // MEAN_LOSS_WINDOW + 1), device=device, dtype=MODEL_DTYPE)\n",
    "    counts = torch.zeros(((upper_bound // batch_size) // MEAN_LOSS_WINDOW + 1), device=device, dtype=torch.int64)\n",
    "    loss_stride = batch_size * MEAN_LOSS_WINDOW\n",
    "\n",
    "    for i in t_range:\n",
    "        r = t[i:i+batch_size]\n",
    "        j = i // loss_stride\n",
    "        losses[j] += train_step(r, env, actor_net_train, loss, optim)\n",
    "        counts[j] += 1\n",
    "        pbar.update(1)\n",
    "\n",
    "    mean_losses = losses / counts\n",
    "    \n",
    "    return mean_losses\n",
    "\n",
    "\n",
    "def train(batch_size, env, actor_net_train, loss, optim, epochs, scheduler=None):\n",
    "    logs = defaultdict(list)\n",
    "    pbar = tqdm(total=epochs, position=0)\n",
    "\n",
    "    upper_bound = env.dates.shape[0] * env.steps * env.n_position\n",
    "    pbar_inner = tqdm(total=upper_bound // batch_size, position=1)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        pbar_inner.reset()\n",
    "        mean_losses = train_epoch(batch_size, env, actor_net_train, loss, optim, pbar_inner)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            rewards = env.rollout_all(actor_net, env.steps + 1)\n",
    "            sortino = ifera.sortino(rewards)\n",
    "            logs[\"sortino\"].append(sortino.item())\n",
    "            logs[\"reward\"].append(rewards.mean().item())\n",
    "            logs[\"meanloss\"].extend(mean_losses.cpu().numpy().tolist())\n",
    "        \n",
    "        pbar.set_description(f\"Sortino: {logs['sortino'][-1]: 4.4f}, Reward: {logs['reward'][-1]: 4.4f}\")\n",
    "        pbar.update(1)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "    return logs\n",
    "\n",
    "\n",
    "#actor_net_train = ifera.ActorNet(env, D_MODEL, actornet_hidden)\n",
    "optim = torch.optim.Adam(actor_net.parameters(), LR)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optim, EXP_LR_DECAY_PER_EPOCH)\n",
    "\n",
    "logs = train(BATCCH_SIZE, env, actor_net, loss, optim, EPOCHS, scheduler=scheduler)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(logs[\"meanloss\"])\n",
    "plt.title(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(logs[\"reward\"])\n",
    "plt.title(\"Reward\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(logs[\"sortino\"])\n",
    "plt.title(\"Sortino\")\n",
    "plt.show()\n",
    "\n",
    "# Convert defaultdict to dict before saving\n",
    "logs_dict = dict(logs)\n",
    "\n",
    "# Get all files in the logs directory\n",
    "files = os.listdir('logs')\n",
    "\n",
    "# Extract version numbers from file names\n",
    "version_numbers = [int(re.search(r'(\\d{6})', file).group(1)) for file in files if re.search(r'(\\d{6})', file)]\n",
    "\n",
    "# Get the max version number, or 0 if no version numbers were found\n",
    "max_version = max(version_numbers) if version_numbers else 0\n",
    "\n",
    "# Increment the max version number by 1\n",
    "new_version = max_version + 1\n",
    "\n",
    "# Format the new version number as a 6-digit string\n",
    "new_version_str = str(new_version).zfill(6)\n",
    "\n",
    "# Create the new file name\n",
    "new_file_name = f'logs_{new_version_str}.txt'\n",
    "\n",
    "# Save the logs to the new file\n",
    "with open(f'logs/{new_file_name}', 'w') as file:\n",
    "    file.write(json.dumps(logs_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_bound = env.dates.shape[0] * env.steps * env.n_position\n",
    "upper_bound - 128, torch.arange(0, upper_bound-128, 128, device=device, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_manager.optimal_values[:, 0, MAX_UNITS].mean(), value_manager.optimal_values[:, 0, MAX_UNITS].std(), value_manager.optimal_values[:, 0, MAX_UNITS].min(), value_manager.optimal_values[:, 0, MAX_UNITS].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = env.rollout_all(actor_net, env.steps + 1)\n",
    "\n",
    "rewards.mean(), rewards.std(), rewards.min(), rewards.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "with torch.no_grad():\n",
    "    upper_bound = env.dates.shape[0] * env.steps * env.n_position\n",
    "    t1 = torch.randperm(upper_bound, device=device, dtype=torch.int64)\n",
    "    t_range = torch.arange(0, upper_bound - batch_size, batch_size, device=device, dtype=torch.int64)\n",
    "    pbar = tqdm.tqdm(t_range)\n",
    "    losses = torch.zeros(upper_bound, device=device, dtype=torch.float32)\n",
    "\n",
    "    for i in pbar:\n",
    "        r = t1[i:i+batch_size]\n",
    "        date_idx, time_idx, pos_idx = r // (env.steps * env.n_position), (r % (env.steps * env.n_position)) // env.n_position, r % env.n_position\n",
    "        x = actor_net_train(date_idx, time_idx + env.start_time_idx, pos_idx)\n",
    "        l = loss(date_idx, time_idx, pos_idx, x)\n",
    "        losses[i:i+batch_size] = l\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_pos = torch.zeros(env.steps, device=device, dtype=torch.int64)\n",
    "\n",
    "with torch.no_grad():\n",
    "    date_idx = torch.tensor([200], device=device, dtype=torch.int64)\n",
    "    time_idx = torch.tensor([0], device=device, dtype=torch.int64)\n",
    "    pos_idx = torch.tensor([5], device=device, dtype=torch.int64)\n",
    "\n",
    "    while time_idx < env.steps - 1:\n",
    "        p = actor_net_train(date_idx, time_idx + env.start_time_idx, pos_idx)\n",
    "\n",
    "        dist = torch.distributions.Categorical(probs=p)\n",
    "        target_pos[time_idx] = dist.mode - env.max_units\n",
    "        pos_idx = dist.mode\n",
    "        time_idx += 1\n",
    "\n",
    "target_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "with torch.no_grad():\n",
    "    price_data = actor_net_train.env_out_transform.price_data_window(date_idx, time_idx)\n",
    "    one_hot_pos = F.one_hot(pos_idx, num_classes=11)\n",
    "    time = actor_net_train.env_out_transform.time[date_idx, time_idx].unsqueeze(-1)\n",
    "    one_hot_pos = torch.cat((one_hot_pos, time), dim=-1).to(dtype=actor_net_train.env_out_transform.dtype)\n",
    "    data_out = actor_net_train.env_out_transform.lin_data(price_data)\n",
    "    \n",
    "\n",
    "optimal_values = loss.value_manager.optimal_action_values(date_idx, time_idx, pos_idx)\n",
    "value_loss = optimal_values.max(dim=1, keepdim=True)[0] - optimal_values\n",
    "weighted_squared_loss = (1.0 * value_loss).pow(2).sum(dim=1)\n",
    "\n",
    "optimal_values.max(), optimal_values.min(), optimal_values.mean(), optimal_values.std(), optimal_values.shape\n",
    "\n",
    "# norm_values = optimal_values / optimal_values.std()\n",
    "\n",
    "# norm_values.max(), norm_values.min(), norm_values.mean(), norm_values.std()\n",
    "\n",
    "loss.value_manager.optimal_values[:,0,:].max(), loss.value_manager.optimal_values[:,0,:].min(), loss.value_manager.optimal_values[:,0,:].mean(), loss.value_manager.optimal_values[:,0,:].std()\n",
    "\n",
    "torch.unravel_index(loss.value_manager.optimal_values[:,:-1,:].argmax(), loss.value_manager.optimal_values[:,:-1,:].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin = nn.Linear(actor_net_train.env_out_transform.price_data_window.n_channels, actor_net_train.env_out_transform.out_channels, device=actor_net_train.env_out_transform.device, dtype=actor_net_train.env_out_transform.dtype)\n",
    "lin._parameters['weight'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.device(\"cuda:0\") if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "t1 = torch.arange(0, 3, device='cuda', dtype=torch.float32)\n",
    "t2 = torch.arange(0, 10, device='cuda', dtype=torch.float32)\n",
    "\n",
    "#pb1 = tqdm(t1, 'Outer loop', position=0)\n",
    "\n",
    "\n",
    "\n",
    "pb1 = tqdm(total=len(t1), desc='Outer loop', position=0)\n",
    "pb2 = tqdm(total=len(t2), desc='Inner loop', position=1)\n",
    "\n",
    "for i in range(len(t1)):\n",
    "    pb2.reset()\n",
    "\n",
    "    for j in range(len(t2)):\n",
    "        time.sleep(0.3)\n",
    "        pb2.update(1)\n",
    "\n",
    "    pb1.update(1)\n",
    "\n",
    "# with tqdm(total=t1.numel(), position=0, desc='Outer loop') as pb1:\n",
    "#     for i in t1:\n",
    "#         with tqdm(total=t2.numel(), position=1, desc='Inner loop', leave=False) as pb2:\n",
    "#             for j in t2:\n",
    "#                 time.sleep(0.1)\n",
    "#                 pb2.update(1)\n",
    "#         pb1.update(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
